{"cells":[{"cell_type":"code","execution_count":77,"id":"11d98967-544f-4553-b241-134c2544ea71","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:39.071764Z","iopub.status.busy":"2024-12-12T21:29:39.071249Z","iopub.status.idle":"2024-12-12T21:29:42.056714Z","shell.execute_reply":"2024-12-12T21:29:42.055201Z","shell.execute_reply.started":"2024-12-12T21:29:39.071710Z"},"trusted":true},"outputs":[],"source":["import random\n","from math import exp\n","from math import log\n","from collections import defaultdict\n","import numpy as np\n","import time\n","import sys\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import kagglehub\n","from sklearn.preprocessing import OneHotEncoder\n","import torch\n"]},{"cell_type":"code","execution_count":78,"id":"3c398b8f-de78-4f86-8b4d-6a1a68d8fbc0","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:42.059903Z","iopub.status.busy":"2024-12-12T21:29:42.059230Z","iopub.status.idle":"2024-12-12T21:29:42.093764Z","shell.execute_reply":"2024-12-12T21:29:42.092562Z","shell.execute_reply.started":"2024-12-12T21:29:42.059807Z"},"trusted":true},"outputs":[],"source":["\n","\n","class KNN(object):\n","\n","    def __init__(self, numUsers, numItems,batchSize  ,lamI = 6e-2, lamJ = 6e-3, learningRate = 0.1, epochs=1):\n","        self._numUsers = numUsers\n","        self._numItems = numItems\n","        self._lamI = lamI\n","        self._lamJ = lamJ\n","        self._learningRate = learningRate\n","        self._users = set()\n","        self._items = set()\n","        self._Iu = defaultdict(set)\n","        self.C = torch.rand(numItems, numItems)  # Random correlation matrix\n","        self._batchSize = batchSize\n","        self._epochs = epochs\n","\n","        \n","    def predict(self, u, i):\n","        # Predict the score for user u and item i\n","        return torch.matmul(self.C[i], self.C.T[:, u])\n","\n","    \n","    \n","    def sigmoid(self, x):\n","        return 1/(1+np.exp(-x))\n","\n","    def train(self, trainData):\n","        \n","        # correlation matrix\n","        self.C =np.random.rand(self._numItems, self._numItems)  \n","        for l in range(self._numItems):\n","            self.C[l][l] = 0\n","            for n in range(l + 1, self._numItems):\n","                self.C[l][n] = self.C[n][l]\n","\n","\n","        print(\"for loop for items*2 is done.\")\n","        # change batch_size to min(batch-size, len(train))\n","        if len(trainData) < self._batchSize:\n","            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(trainData)))\n","            self._batchSize = len(trainData)\n","                  \n","        self._trainDict, self._users, self._items = self._dataPretreatment(trainData)\n","        N = len(trainData) * self._epochs\n","        users, pItems, nItems = self._sampling(N)\n","        itr = 0\n","        t2 = t0 = time.time()\n","        while (itr+1)*self._batchSize < N:\n","            # print(\"iter : \" ,  itr , \" batch size : \"  , self._batchSize)\n","      \n","            self._mbgd(\n","                \n","                users[itr*self._batchSize: (itr+1)*self._batchSize],\n","                pItems[itr*self._batchSize: (itr+1)*self._batchSize],\n","                nItems[itr*self._batchSize: (itr+1)*self._batchSize]\n","            )\n","            \n","            itr += 1\n","            t2 = time.time()\n","            sys.stderr.write(\"\\rProcessed %s ( %.3f%% ) in %.1f seconds\" %(str(itr*self._batchSize), 100.0 * float(itr*self._batchSize)/N, t2 - t0))\n","            sys.stderr.flush()\n","        if N > 0:\n","            sys.stderr.write(\"\\nTotal training time %.2f seconds; %.2f samples per second\\n\" % (t2 - t0, N*1.0/(t2 - t0)))\n","            sys.stderr.flush()\n","            \n","            \n","    def _mbgd(self, users, pItems, nItems):\n","        \n","        prev = -2**10\n","        for _ in range(30):\n","            \n","            gradientC = defaultdict(float)\n","            obj = 0\n","\n","            for ind in range(len(users)):\n","                u, i, j = users[ind], pItems[ind], nItems[ind]\n","                x_ui = sum([self.C[i][l] for l in self._Iu[u] if i != l])\n","                x_uj = sum([self.C[j][l] for l in self._Iu[u]])\n","                x_uij =  x_ui - x_uj\n","                \n","                for l in self._Iu[u]:\n","                    if l != i:\n","                        gradientC[(i,l)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[i][l]**2\n","                        gradientC[(l,i)] += (1-self.sigmoid(x_uij)) + self._lamI * self.C[l][i]**2\n","                    gradientC[(j,l)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[j][l]**2\n","                    gradientC[(l,j)] += -(1-self.sigmoid(x_uij)) + self._lamJ * self.C[l][j]**2\n","                    \n","                    obj -= 2*self._lamI * self.C[i][l]**2 + 2*self._lamJ * self.C[j][l]**2\n","                    \n","                obj += log(self.sigmoid(x_uij))\n","            \n","            #print 'OBJ: ', obj\n","            if prev > obj: \n","                break\n","            prev = obj\n","            \n","            for a,b in gradientC:\n","                self.C[a][b] += self._learningRate * gradientC[(a,b)]\n","            \n","        #print _, '\\n'\n","        \n","    def _sampling(self, N):\n","        print(f\"Generating {N} random training samples\")\n","        userList = list(self._users)\n","        \n","        # Generate all random numbers at once\n","        userIndex = np.random.randint(0, len(self._users), N)\n","        pItems = []\n","        nItems = []\n","        \n","        # Process in smaller chunks\n","        chunk_size = 1000\n","        for chunk_start in range(0, N, chunk_size):\n","            chunk_end = min(chunk_start + chunk_size, N)\n","            chunk_indices = userIndex[chunk_start:chunk_end]\n","            \n","            for index in chunk_indices:\n","                u = userList[index]\n","                # Get positive item\n","                i = self._trainDict[u][np.random.randint(len(self._Iu[u]))]\n","                pItems.append(i)\n","                \n","                # Get negative item more efficiently\n","                j = np.random.randint(self._numItems)\n","                while j in self._Iu[u]:\n","                    j = np.random.randint(self._numItems)\n","                nItems.append(j)\n","            \n","            print(f\"Generated {chunk_end}/{N} samples ({(chunk_end/N)*100:.1f}%)\")\n","        \n","        print(\"Sampling completed!\")\n","        return userIndex, pItems, nItems\n","    def predictionsKNN(self, K, u):\n","        # Convert the correlation matrix to a PyTorch tensor if it's not already\n","        if not isinstance(self.C, torch.Tensor):\n","            self.C = torch.tensor(self.C, dtype=torch.float32)\n","\n","        # Check if the user has any items\n","        if not self._Iu[u]:\n","            print(f\"User {u} has no items.\")\n","            return torch.zeros(self._numItems)  # Return a zero tensor or handle as needed\n","\n","        if K >= len(self._Iu[u]):\n","            # Use PyTorch sum and indexing\n","            res = torch.sum(torch.stack([self.C[:, l] for l in self._Iu[u]]), dim=0)\n","        else:\n","            res = []\n","            for i in range(self._numItems):\n","                # Use PyTorch operations for sorting and summing\n","                item_scores = torch.tensor([self.C[i][l] for l in self._Iu[u]], dtype=torch.float32)\n","                top_k_scores = torch.topk(item_scores, K).values\n","                res.append(torch.sum(top_k_scores).item())\n","        return res\n","    ...\n","\n","    def prediction(self, u, i):\n","        scores = self.predictionsAll(u)\n","        # Fix the index calculation\n","        return scores[i] > sorted(scores)[int(self._numItems * 0.8)]\n","    \n","\n","    def prediction(self, u, i):\n","        \n","        scores = self.predictionsAll(u)\n","        return scores[i] > sorted(scores)[self._numItem*0.8]\n","\n","    def _dataPretreatment(self, data):\n","        # print(\" control is here\")\n","        dataDict = defaultdict(list)\n","        items = set()\n","        ind = 0\n","        for index , rows in data.iterrows():\n","            ind+=1\n","            u = rows['userid_DI']\n","\n","            i = rows['course_id']\n","            # print(\"user : \" , u , \"item : \" , i)\n","            self._Iu[u].add(i)\n","            dataDict[u].append(i)\n","            items.add(i)\n","\n","        # print(\"data dicts are : \" , dataDict , dataDict.keys , items)\n","        return dataDict, set(dataDict.keys()), items"]},{"cell_type":"code","execution_count":79,"id":"19938cde-a1db-47a3-975b-9ab889f0aa0d","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:42.095732Z","iopub.status.busy":"2024-12-12T21:29:42.095374Z","iopub.status.idle":"2024-12-12T21:29:45.929583Z","shell.execute_reply":"2024-12-12T21:29:45.928507Z","shell.execute_reply.started":"2024-12-12T21:29:42.095698Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   index                   course_id       userid_DI  registered  viewed  \\\n","0      0  HarvardX/CB22x/2013_Spring  MHxPC130442623           1       0   \n","1      1         HarvardX/CS50x/2012  MHxPC130442623           1       1   \n","2      2  HarvardX/CB22x/2013_Spring  MHxPC130275857           1       0   \n","3      3         HarvardX/CS50x/2012  MHxPC130275857           1       0   \n","4      4  HarvardX/ER22x/2013_Spring  MHxPC130275857           1       0   \n","\n","   explored  certified final_cc_cname_DI LoE_DI  YoB  ... grade start_time_DI  \\\n","0         0          0     United States    NaN  NaN  ...     0    2012-12-19   \n","1         0          0     United States    NaN  NaN  ...     0    2012-10-15   \n","2         0          0     United States    NaN  NaN  ...     0    2013-02-08   \n","3         0          0     United States    NaN  NaN  ...     0    2012-09-17   \n","4         0          0     United States    NaN  NaN  ...     0    2012-12-19   \n","\n","  last_event_DI nevents  ndays_act  nplay_video  nchapters  nforum_posts  \\\n","0    2013-11-17     NaN        9.0          NaN        NaN             0   \n","1           NaN     NaN        9.0          NaN        1.0             0   \n","2    2013-11-17     NaN       16.0          NaN        NaN             0   \n","3           NaN     NaN       16.0          NaN        NaN             0   \n","4           NaN     NaN       16.0          NaN        NaN             0   \n","\n","   roles  incomplete_flag  \n","0    NaN              1.0  \n","1    NaN              1.0  \n","2    NaN              1.0  \n","3    NaN              1.0  \n","4    NaN              1.0  \n","\n","[5 rows x 21 columns]\n"]}],"source":["# path2 = kagglehub.dataset_download('thedevastator/online-course-student-engagement-metrics', path='Courses.csv')\n","# print(\"path 2 :  \" , path2)\n","\n","path2 = \"Courses.csv\"\n","df = pd.read_csv(path2)  # Windows-1252 encoding\n","print(df.head())"]},{"cell_type":"code","execution_count":80,"id":"606d8476-5452-4e17-a2bb-f3bf0c39f5c7","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:45.932219Z","iopub.status.busy":"2024-12-12T21:29:45.931874Z","iopub.status.idle":"2024-12-12T21:29:45.951099Z","shell.execute_reply":"2024-12-12T21:29:45.949911Z","shell.execute_reply.started":"2024-12-12T21:29:45.932186Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                    course_id       userid_DI\n","0  HarvardX/CB22x/2013_Spring  MHxPC130442623\n","1         HarvardX/CS50x/2012  MHxPC130442623\n","2  HarvardX/CB22x/2013_Spring  MHxPC130275857\n","3         HarvardX/CS50x/2012  MHxPC130275857\n","4  HarvardX/ER22x/2013_Spring  MHxPC130275857\n","(641138, 2)\n"]}],"source":["df2 = df[[\"course_id\"  , \"userid_DI\"]]\n","print(df2.head())\n","print(df2.shape)"]},{"cell_type":"code","execution_count":81,"id":"edd10a12-b353-4d42-923e-34bf7c318f89","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:45.952930Z","iopub.status.busy":"2024-12-12T21:29:45.952422Z","iopub.status.idle":"2024-12-12T21:29:46.637597Z","shell.execute_reply":"2024-12-12T21:29:46.636377Z","shell.execute_reply.started":"2024-12-12T21:29:45.952882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0\n","1 0\n","0 1\n","1 1\n","2 1\n","3 1\n","4 1\n","0 2\n","0 3\n","1 3\n","2 3\n","2 4\n","0 5\n","1 5\n","2 5\n","\n","Users and their course counts:\n","userid_DI\n","476518    1\n","476519    1\n","476520    1\n","476521    1\n","476522    1\n","476523    1\n","476524    1\n","476525    1\n","476528    1\n","12        1\n","Name: course_id, dtype: int64\n","\n","Summary statistics:\n","Average courses per user: 1.35\n","Median courses per user: 1.00\n","Max courses by a user: 16\n","Min courses by a user: 1\n"]}],"source":["#  Create mapping dictionaries\n","course_id_map = {id_: idx for idx, id_ in enumerate(df2['course_id'].unique())}\n","user_id_map = {id_: idx for idx, id_ in enumerate(df2['userid_DI'].unique())}\n","\n","# Create new dataframe with mapped values\n","df2_numeric = df2.copy()\n","df2_numeric['course_id'] = df2['course_id'].map(course_id_map)\n","df2_numeric['userid_DI'] = df2['userid_DI'].map(user_id_map)\n","\n","# Print first 15 rows of numeric data\n","i = 0\n","for index, rows in df2_numeric.iterrows():\n","    i += 1\n","    if(i > 15):\n","        break\n","    print(rows['course_id'], rows['userid_DI'])\n","\n","\n","user_course_counts = df2_numeric.groupby('userid_DI')['course_id'].nunique()\n","\n","# Print users with their course counts, sorted by count\n","print(\"\\nUsers and their course counts:\")\n","print(user_course_counts.sort_values().head(10))  # Show 10 users with least courses\n","print(\"\\nSummary statistics:\")\n","print(f\"Average courses per user: {user_course_counts.mean():.2f}\")\n","print(f\"Median courses per user: {user_course_counts.median():.2f}\")\n","print(f\"Max courses by a user: {user_course_counts.max()}\")\n","print(f\"Min courses by a user: {user_course_counts.min()}\")\n","# Create train/test splits following leave-one-out evaluation scheme\n","def create_train_test_split(df):\n","    print(\"creating train-test split\")\n","    \n","    # Get random indices for test set - one per user\n","    test_indices = df.groupby('userid_DI').apply(\n","        lambda x: x.sample(n=1).index[0]\n","    ).values\n","    \n","    # Create test and train dataframes using boolean indexing\n","    test_df = df.loc[test_indices]\n","    train_df = df.drop(test_indices)\n","    \n","    print(\"done with test-train split\")\n","    \n","    return train_df, test_df\n","\n","# Create evaluation pairs E(u) for each user\n","def create_evaluation_pairs(train_df, test_df, num_items, sample_size=100):\n","\n","    print(\"creating evaluation pairs\")\n","    evaluation_pairs = {}\n","    \n","    # Pre-compute test user data\n","    test_user_items = test_df.set_index('userid_DI')['course_id']\n","    train_user_items = train_df.groupby('userid_DI')['course_id'].apply(set).to_dict()\n","    all_items = set(range(num_items))\n","    \n","    # Process each user more effidef predictions_to_courses(predictions):\n","#     predictions_int = [int(round(p)) for p in predictions]\n","#     return [course_id_map[i] for i in predictions_int]\n","\n","# print(\"predictions to courses:\", predictions_to_courses(predictions))ciently\n","    for user in test_df['userid_DI'].unique():\n","        positive_item = test_user_items[user]\n","        train_items = train_user_items.get(user, set())\n","        \n","        # Use set operations for faster filtering\n","        negative_items = list(all_items - train_items - {positive_item})\n","        \n","        # Optimize random sampling\n","        if len(negative_items) > sample_size:\n","            # Use numpy's efficient random sampling\n","            negative_items = np.random.choice(\n","                negative_items, \n","                size=sample_size,\n","                replace=False\n","            )\n","            \n","        evaluation_pairs[user] = {\n","            'positive_item': positive_item,\n","            'negative_items': negative_items\n","        }\n","\n","    print(\"done with evaluation pairs\")\n","    \n","    return evaluation_pairs\n","\n","# Calculate AUC using PyTorch\n","def calculate_auc_pytorch(model, eval_pairs, device):\n","    auc_sum = 0.0\n","    user_count = 0\n","\n","    for user, items in eval_pairs.items():\n","        positive_item = items['positive_item']\n","        negative_items = items['negative_items']\n","        \n","        # Move data to GPU\n","        user_tensor = torch.tensor([user], device=device)\n","        positive_item_tensor = torch.tensor([positive_item], device=device)\n","        negative_items_tensor = torch.tensor(negative_items, device=device)\n","\n","        # Predict scores\n","        x_ui = model.predict(user_tensor, positive_item_tensor)\n","        x_uj = model.predict(user_tensor.repeat(len(negative_items)), negative_items_tensor)\n","\n","        # Vectorized comparison\n","        auc_user = torch.sum(x_ui > x_uj).item() / len(negative_items)\n","        auc_sum += auc_user\n","        user_count += 1\n","\n","    return auc_sum / user_count if user_count > 0 else 0.0\n","\n","\n","# # splitting into training and testing data\n","# train_df, test_df = create_train_test_split(df2_numeric)\n","\n","# # Creating evaluation pairs for calculation of auc\n","# eval_pairs = create_evaluation_pairs(train_df, test_df, num_items=16, sample_size=100)\n","\n"]},{"cell_type":"code","execution_count":82,"id":"9e71bdbe-e821-4444-a01e-aa606db6114a","metadata":{"execution":{"iopub.execute_input":"2024-12-12T21:29:59.945135Z","iopub.status.busy":"2024-12-12T21:29:59.944763Z","iopub.status.idle":"2024-12-12T21:31:04.381110Z","shell.execute_reply":"2024-12-12T21:31:04.379545Z","shell.execute_reply.started":"2024-12-12T21:29:59.945103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["batch size: 64113\n","total samples: 641138\n","User 476518 took 1 courses\n","for loop for items*2 is done.\n","Generating 641138 random training samples\n","Generated 1000/641138 samples (0.2%)\n","Generated 2000/641138 samples (0.3%)\n","Generated 3000/641138 samples (0.5%)\n","Generated 4000/641138 samples (0.6%)\n","Generated 5000/641138 samples (0.8%)\n","Generated 6000/641138 samples (0.9%)\n"]}],"source":["# Initialize the KNN model\n","# no of UI pairs = 641138\n","\n","# Initialize the KNN model\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Train the model with smaller parameters\n","epochs = 1\n","batch_size =(int)( 641138/10)  # Smaller batch size\n","print(\"batch size:\", batch_size)\n","\n","\n","\n","\n","# Significantly reduce the data size for testing\n","sample_size = (int) (641138)  # Take only 10k samples from the dataset\n","df_sample = df2_numeric.sample(n=sample_size, random_state=42)\n","print(\"total samples:\", sample_size)\n","num_users = 476532\n","num_items = 16\n","bpr = KNN(num_users, num_items , epochs, batch_size)\n","\n","user = 476518\n","print(f\"User {user} took {len(df_sample[df_sample['userid_DI'] == user])} courses\")\n","\n","try:\n","    bpr.train(df_sample)\n","except Exception as e:\n","    print(f\"Error during training: {e}\")\n","\n","# Make predictions\n","K = 3\n","user = 476518\n","\n","\n","predictions = bpr.predictionsKNN(K, user)\n","\n","print(\"predictions:\", predictions)\n","\n","# def predictions_to_courses(predictions):\n","#     predictions_int = [int(round(p)) for p in predictions]\n","#     return [course_id_map[i] for i in predictions_int]\n","\n","# print(\"predictions to courses:\", predictions_to_courses(predictions))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4201670,"sourceId":7251808,"sourceType":"datasetVersion"},{"datasetId":5828270,"sourceId":9563586,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}
